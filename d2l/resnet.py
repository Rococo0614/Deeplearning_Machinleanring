import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torch.optim as optim
from torchvision import datasets,transforms
import time
import matplotlib.pyplot as plt


transform = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(), #å›¾å¹…å€¼è½¬æ¢åˆ°0-1
    transforms.Normalize((0.5,),(0.5,)) #çŸ¥é“æ˜¯ä»0-1å˜æ¢ä¸º-1-1,ä½†æ˜¯ä¸ºä»€ä¹ˆ0.5,?è½¬æ¢æˆå…ƒç»„,åªæœ‰åœ¨ å•å…ƒç´ å…ƒç»„ çš„æ—¶å€™ï¼Œå¿…é¡»å†™ä¸Šå°¾éšé€—å·ï¼Œå¦åˆ™ Python ä¼šæŠŠå®ƒå½“ä½œæ™®é€šçš„æ•°å­—

])

train_dataset = datasets.FashionMNIST(root= 'data', train = True, download = False, transform = transform)
test_dataset = datasets.FashionMNIST(root = 'data', train = False, download = False, transform = transform)

train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True, num_workers = 2, pin_memory = True)
test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False, num_workers = 2, pin_memory = True)

classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



def train(model,loader,optimizer,criterion,device):
    model.train()
    total_loss, correct = 0,0
    for X,y in loader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(X)
        loss = criterion(out,y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()*X.size(0)
        correct += (out.argmax(1) == y).sum().item()
    return total_loss/len(loader.dataset),correct/len(loader.dataset)

def test(model,loader,criterion,device):
    model.eval()
    total_loss,correct = 0, 0
    with torch.no_grad():
        for X,y in loader:
            X,y = X.to(device), y.to(device)
            out = model(X)
            loss = criterion(out,y)
            total_loss += loss.item()*X.size(0)
            correct += (out.argmax(1) == y).sum().item()
    return total_loss / len(loader.dataset), correct / len(loader.dataset)


class BasicBlock(nn.Module):
    expansion = 1 #å®šä¹‰çš„è¿™ä¸ªé‡æ˜¯ä¸ºäº†è¡¨ç¤ºæ®‹å·®å—è¾“å‡ºé€šé“æ•°å’Œä¸»åˆ†æ”¯å·ç§¯å±‚è¾“å‡ºé€šé“æ•°ä¹‹é—´çš„æ¯”ä¾‹ä¸­é—´é€šé“æ•°å°ï¼Œè®¡ç®—å¿«ï¼›æœ€ç»ˆé€šé“æ•°å¤§ï¼Œè¡¨è¾¾åŠ›å¼ºã€‚åšæƒè¡¡
    def __init__(self,in_channels,out_channels,stride=1): #åˆå§‹åŒ–ä¹Ÿä¸ä¸€æ ·ï¼Œè¿™é‡ŒæŠŠé€šé“æ•°ç›®ç›´æ¥åˆ—åœ¨äº†å‡½æ•°é‡Œé¢
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size =3,
                               stride = stride,padding = 1, bias = False) #biasåç½®
        self.bn1 = nn.BatchNorm2d(out_channels) #è¿˜æ˜¯å½’ä¸€åŒ–ï¼Œå¯ä»¥æœ‰æ•ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸/æ¢¯åº¦æ¶ˆå¤±ï¼ŒåŒæ—¶åŠ å¿«æ”¶æ•›é€Ÿåº¦
        self.conv2 = nn.Conv2d(out_channels, out_channels,kernel_size = 3,
                                stride = 1, padding = 1 ,bias = False)
        self.bn2 = nn.BatchNorm2d(out_channels) #å¦ä¸€å±‚å½’ä¸€åŒ–

        self.shortcut = nn.Sequential() #è¿™è¿˜æ˜¯å¤´ä¸€æ¬¡ç¢°åˆ°åœ¨è¿™é‡Œä»€ä¹ˆéƒ½ä¸å¹²
        if stride != 1 or in_channels != out_channels * self.expansion: #ä¸ºä»€ä¹ˆä¼šä¸æ˜¯1å‘¢ï¼Œä¿è¯å½¢çŠ¶å¯¹é½ï¼Œè·Ÿbottleneckä¹Ÿæœ‰å…³ç³»
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, self.expansion*out_channels,
                kernel_size = 1,stride = stride, bias = False),
                nn.BatchNorm2d(self.expansion*out_channels)
            )
    
    def forward(self,x):
        out = F.relu(self.bn1(self.conv1(x))) #è¿™ä¸ªå’Œå¤–é¢çš„ReLUæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿæ²¡å•¥åŒºåˆ«
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x) #æ®‹å·®åŠ æ³•
        out = F.relu(out)
        return out

class ResNet(nn.Module):
    def __init__(self,block,num_blocks,num_classes = 10): #æ‰€ä»¥æ˜¯ä¸æ˜¯æˆ‘ä¸­é—´å†å­¦ä¸€æ­¥VGGä¼šæ›´å¥½ï¼Ÿ
        super().__init__()
        self.in_channels = 64
        #1x224x224 out: 64x112x112
        self.conv1 = nn.Conv2d(1,64,kernel_size = 7, stride = 2,
                              padding = 3, bias = False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
        self.layer1 = self._make_layer(block,64,num_blocks[0],stride = 1) #è¿™æ˜¯å¡äº†å‡ ä¸ªå—è¿›æ¥
        self.layer2 = self._make_layer(block,128,num_blocks[1], stride = 2) #æ­¥é•¿ä¸ºä»€ä¹ˆä¼šå˜åŒ–?
        self.layer3 = self._make_layer(block,256,num_blocks[2],stride = 2)
        self.layer4 = self._make_layer(block,512,num_blocks[3],stride = 2)
        self.avgpool = nn.AdaptiveAvgPool2d((1,1)) #è¿™é‡Œçš„è¾“å…¥ä»£è¡¨ä»€ä¹ˆï¼Ÿ
        self.fc = nn.Linear(512*block.expansion,num_classes)

    def _make_layer(self,block,out_channels,num_blocks,stride):
        strides = [stride] + [1] * (num_blocks - 1) #è¿™é‡Œçš„æ•°å­—åœ¨ä¸­æ‹¬å·é‡Œ
        layers = []
        for s in strides:
            layers.append(block(self.in_channels,out_channels,s)) #blockå‡½æ•°æ˜¯å“ªé‡Œæ¥çš„?
            self.in_channels = out_channels * block.expansion
        return nn.Sequential(*layers) #layersæ˜¯ä¸€ä¸ªé‡Œé¢åŒ…å«blockçš„ä¸œè¥¿
    
    def forward(self,x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.pool(out)
        out = self.layer1(out) #ä»–æŠŠçº¿æ€§å±‚ç®€åŒ–ï¼Œææˆè¿™ä¸ªæ ·
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = self.avgpool(out)
        out = torch.flatten(out, 1)
        out = self.fc(out)
        return out
    
def ResNet18(num_classes = 10):
        return ResNet(BasicBlock,[2,2,2,2],num_classes) #æ¯ä¸ªlayeré‡Œblockçš„æ•°é‡

model = ResNet18().to(device)  
model = ResNet18().to(device)
try:
    model.load_state_dict(torch.load("resnet18_fashionmnist.pth"))
    print("æˆåŠŸåŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹ âœ…")
except:
    print("æœªæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒ ğŸš€")
criterion = nn.CrossEntropyLoss()
num_epochs = 20
optimizer = optim.Adam(model.parameters(), lr = 0.001)  
scheduler = optim.lr_scheduler.StepLR(optimizer,step_size = 5, gamma = 0.1)


train_losses, train_accs, test_losses, test_accs = [],[],[],[]
start_time = time.time()
for epoch in range(1, num_epochs + 1):
    t0 = time.time()
    train_loss,train_acc = train(model,train_loader,optimizer,criterion,device)
    test_loss,test_acc = test(model,test_loader,criterion,device)
    scheduler.step()

    train_losses.append(train_loss)
    train_accs.append(train_acc)
    test_losses.append(test_loss)
    test_accs.append(test_acc)

    t1 = time.time()
    print(f"Epoch {epoch:2d}/{num_epochs} "
    f"train_loss = {train_loss:.4f} train_acc = {train_acc:.4f} "
    f"test_loss = {test_loss:.4f} test_acc = {test_acc:.4f} " 
    f"time = {(t1-t0):.1f}s")

total_time = time.time() - start_time
print(f"time = {total_time:.1f}s")


    

# -------------------------
# å¯è§†åŒ–è®­ç»ƒæ›²çº¿ï¼ˆåœ¨ notebook ä¸­ä½¿ç”¨ %matplotlib inlineï¼‰
# -------------------------
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(train_losses, label="train loss")
plt.plot(test_losses, label="test loss")
plt.legend()
plt.title("Loss")

plt.subplot(1,2,2)
plt.plot(train_accs, label="train acc")
plt.plot(test_accs, label="test acc")
plt.legend()
plt.title("Accuracy")
plt.tight_layout()
plt.show()

# -------------------------
# éšæœºå±•ç¤ºè‹¥å¹²æµ‹è¯•é¢„æµ‹ç»“æœ
# -------------------------
import random
model.eval()
X, y = next(iter(test_loader))
X, y = X.to(device), y.to(device)
logits = model(X)
preds = logits.argmax(dim=1)
# å–å‰ 8 å¼ 
n_show = 8
imgs = X[:n_show].cpu()
preds = preds[:n_show].cpu().numpy()
labels = y[:n_show].cpu().numpy()

plt.figure(figsize=(12,3))
for i in range(n_show):
    plt.subplot(1, n_show, i+1)
    plt.imshow(imgs[i].squeeze(), cmap='gray')
    plt.title(f"P:{classes[preds[i]]}\nT:{classes[labels[i]]}")
    plt.axis('off')
plt.show()

torch.save(model.state_dict(), "resnet18_fashionmnist.pth")
print("æ¨¡å‹å·²ä¿å­˜ âœ…")